# 深度学习 笔记

## 概念

### 前向传播

forward propagation/ pass 

按照输入层 ==> 输出层的顺序去计算和存储神经网络中每层的结果。



### 反向传播

backward / back propagation

指的是**计算神经网络参数梯度的方法**。 该方法根据微积分中的链式规则，
按相反的顺序从输出层 ==> 输入层遍历网络。该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。

###  softmax函数 

softmax函数，又称**归一化指数函数。**它是二分类函数[sigmoid](https://so.csdn.net/so/search?q=sigmoid&spm=1001.2101.3001.7020)在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。可以将各个输出节点的输出值范围映射到[0, 1]，并且约束各个输出节点的输出值的和为1的函数

$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$$

下面为大家解释一下为什么softmax是这种形式。

首先，我们知道概率有两个性质：1）预测的概率为非负数；2）各种预测结果概率之和等于1。

softmax就是将在负无穷到正无穷上的预测结果按照这两步转换为概率的。

**1）将预测结果转化为非负数**

下图为y=exp(x），即 $y = e^x$ 的图像，我们可以知道指数函数的值域取值范围是零到正无穷。**softmax第一步就是将模型的预测结果转化到指数函数上，这样保证了概率的非负性**



**2）各种预测结果概率之和等于1**

为了确保各个预测结果的概率之和等于1。我们只需要将转换后的结果进行**归一化处理**。方法就是将转化后的结果除以所有转化后结果之和，可以理解为转化后结果占总数的百分比。这样就得到近似的概率。

详见 [cdsn这篇文章](https://blog.csdn.net/lz_peter/article/details/84574716)



### todo

在多分类问题中，我们经常使用交叉熵作为损失函数。

